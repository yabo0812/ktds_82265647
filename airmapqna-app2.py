import os
import streamlit as st
from dotenv import load_dotenv
from datetime import datetime

# OpenAI ë° Azure ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from openai import AzureOpenAI
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Azure OpenAI (RAG ë° ì„ë² ë”©ìš©)
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_EMBBEDING_MODEL_NAME = os.getenv("AZURE_OPENAI_EMBBEDING_MODEL_NAME")
AZURE_OPENAI_EMBBEDING_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_EMBBEDING_DEPLOYMENT_NAME")
AZURE_OPENAI_EMBBEDING_API_VERSION = os.getenv("AZURE_OPENAI_EMBBEDING_API_VERSION")
AZURE_OPENAI_CHAT_MODEL_NAME = os.getenv("AZURE_OPENAI_CHAT_MODEL_NAME")
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME")

# Azure AI Search (RAG ê²€ìƒ‰ìš©)
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
AZURE_SEARCH_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_INDEX_NAME = os.getenv("AZURE_SEARCH_INDEX_NAME2")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì—ì–´ë§µ ìš´ì˜ Q&A",
    page_icon="â˜€ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì‚¬ìš©ì ì •ì˜ CSS - ë” ê¹”ë”í•œ ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 0.5rem;
        background: linear-gradient(90deg, #066CEA 0%, #18CBEB 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
            
    .main-header h1 {
        font-size: 2rem;
    }            
    
    [data-testid="stSidebar"] {
        background-color: #F3F3EE; 
    }
    
    /* ì±„íŒ… ì…ë ¥ ì˜ì—­ ê³ ì • */
    .stChatInput > div {
        position: sticky;
        bottom: 0;
        z-index: 1000;
    }
    
    /* ìŠ¤í¬ë¡¤ ìµœì í™” */
    .element-container {
        max-height: none;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def load_clients():
    """
    Azure ë° OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤.
    """
    try:
        # 1. Azure OpenAI í´ë¼ì´ì–¸íŠ¸ (RAGìš©)
        azure_openai_client = AzureOpenAI(
            api_version=AZURE_OPENAI_EMBBEDING_API_VERSION,
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            api_key=AZURE_OPENAI_API_KEY,
        )
        
        # 2. Azure AI Search í´ë¼ì´ì–¸íŠ¸ (RAG ê²€ìƒ‰ìš©)
        search_client = SearchClient(
            endpoint=AZURE_SEARCH_ENDPOINT,
            index_name=AZURE_SEARCH_INDEX_NAME,
            credential=AzureKeyCredential(AZURE_SEARCH_KEY)
        )

        # 3. Azure OpenAI í´ë¼ì´ì–¸íŠ¸ (ì™¸ë¶€ê²€ìƒ‰ìš©)
        openai_client = AzureOpenAI(
            api_version=AZURE_OPENAI_EMBBEDING_API_VERSION,
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            api_key=AZURE_OPENAI_API_KEY,
        )
        
        return azure_openai_client, search_client, openai_client
    except (ValueError, TypeError, KeyError, Exception) as e:
        st.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None, None

def route_question(query):
    """
    ì§ˆë¬¸ì˜ ì¢…ë¥˜ë¥¼ íŒë‹¨í•˜ì—¬ ì²˜ë¦¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    query_lower = query.lower()

    if "ë¦¬ëˆ…ìŠ¤" in query_lower or "linux" in query_lower:
        return "linux"
    elif "í¬ìŠ¤íŠ¸ê·¸ë ˆ" in query_lower or "í¬ìŠ¤íŠ¸ê·¸ë ˆìŠ¤" in query_lower or "postgres" in query_lower or "postgresql" in query_lower:
        return "postgres"
    else:
        return "wiki"

def get_rag_response(query, azure_openai_client, search_client):
    """
    Azure AI Searchì™€ Azure OpenAIë¥¼ ì‚¬ìš©í•´ RAG ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ë²¡í„° ê²€ìƒ‰ìš©)
        embedding_response = azure_openai_client.embeddings.create(
            input=[query],
            model=AZURE_OPENAI_EMBBEDING_DEPLOYMENT_NAME
        ).data[0].embedding
        
        vector_query = VectorizedQuery(vector=embedding_response, k_nearest_neighbors=3, fields="content_embedding")

        # 2. Azure AI Searchì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + ë²¡í„°)
        results = search_client.search(
            search_text=query,
            vector_queries=[vector_query],
            select=["document_title", "content_text"],
            top=10
        )

        # 3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        formatted_results = []
        for result in results:            
            title = result.get("document_title", "ì œëª© ì—†ìŒ")
            content = result.get("content_text", "")
            score = result.get('@search.rerank_score', result.get('@search.score', 0.0))
            if content:
                formatted_results.append(
                    f"[ë¬¸ì„œ ì •ë³´]\n"
                    f"ì œëª©: {title}\n"
                    f"ê´€ë ¨ì„± ì ìˆ˜: {score:.4f}\n\n"
                    f"[ë‚´ìš©]\n{content}...\n"
                )
        context = "\n\n---\n\n".join(formatted_results)
        
        if not context:
            return "ê´€ë ¨ëœ ìœ„í‚¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 4. LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_message = """
        ë‹¹ì‹ ì€ ì‚¬ë‚´ ìœ„í‚¤ ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤.
        ì•„ë˜ì— ì œê³µëœ ìœ„í‚¤ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ê³ , "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
        """
        
        response = azure_openai_client.chat.completions.create(
            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": f"## ìœ„í‚¤ ë¬¸ì„œ:\n{context}\n\n## ì§ˆë¬¸:\n{query}"}
            ],
            temperature=0.1
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤, RAG ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.: {e}"

def get_external_response(query, topic, openai_client):
    """
    Azure OpenAI APIë¥¼ ì‚¬ìš©í•´ íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ì¼ë°˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    topic_map = {
        "linux": "ë‹¹ì‹ ì€ ë¦¬ëˆ…ìŠ¤ ëª…ë ¹ì–´ì™€ ì‰˜ ìŠ¤í¬ë¦½íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        "postgres": "ë‹¹ì‹ ì€ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ íŠœë‹ ë° SQL ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
    }
    system_message = topic_map.get(topic, "ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
    system_message += " ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì •í™•í•œ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ì œê³µí•´ ì£¼ì„¸ìš”. í•„ìš”í•œ ê²½ìš° ì½”ë“œ ì˜ˆì‹œë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”."
    system_message += " ë‹¹ì‹ ì˜ ì „ë¬¸ë¶„ì•¼ ì™¸ì˜ ì •ë³´ëŠ” ì „í˜€ ëª¨ë¦…ë‹ˆë‹¤."

    try:
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": query}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤, ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.: {e}"

def generate_response(query, clients):
    """ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    aoai_client, search_client, openai_client = clients
    
    topic = route_question(query)
    
    if topic == "wiki":
        return get_rag_response(query, aoai_client, search_client)
    else:
        return get_external_response(query, topic, openai_client)

def main():
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>ì—ì–´ë§µ ìš´ì˜ Q&A</h1>
        <p>ë¦¬ëˆ…ìŠ¤, PostgreSQL, ìœ„í‚¤ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</p>
    </div>
    """, unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("â˜€ï¸ ì—ì–´ë§µ ìš´ì˜ Q&A")
        st.info("ì—ì–´ë§µ ìœ„í‚¤(Confluence) ë‚´ìš© ê¸°ë°˜ì˜ ìš´ì˜ ì´ë ¥ì„ ê²€ìƒ‰í•˜ê³ , ë¦¬ëˆ…ìŠ¤ ë° PostgreSQL ê´€ë ¨ ìš´ì˜ì‹œ í•„ìš”í•œ ê²€ìƒ‰ì— í™œìš©í•©ë‹ˆë‹¤.")
        
        st.header("âš™ï¸ì„¤ì •")
        if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.rerun()
        
        st.header("ğŸ” ì‚¬ìš© íŒ")
        st.write("âœ“&nbsp;&nbsp;ë¦¬ëˆ…ìŠ¤ ê´€ë ¨ ì§ˆë¬¸ì‹œ 'linux' ë˜ëŠ” 'ë¦¬ëˆ…ìŠ¤' í¬í•¨")
        st.write("âœ“&nbsp;&nbsp;PostgreSQL ê´€ë ¨ ì§ˆë¬¸ì‹œ 'postgres' ë˜ëŠ” 'postgresql' í¬í•¨")
        st.write("âœ“&nbsp;&nbsp;ê·¸ì™¸ í‚¤ì›Œë“œì— ëŒ€í•œ ì§ˆë¬¸ì€ ìœ„í‚¤ì—ì„œë§Œ ê²€ìƒ‰ë˜ê³ , ë‹¤ë¥¸ ì •ë³´ ê²€ìƒ‰ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ
    clients = load_clients()
    if not all(clients):
        st.error("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì´ˆê¸° ë©”ì‹œì§€ ì¶”ê°€ (ì²« ì‹¤í–‰ ì‹œì—ë§Œ)
    if not st.session_state.messages:
        st.session_state.messages.append({
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ì—ì–´ë§µ ì„œë¹„ìŠ¤ ìš´ì˜í•˜ë©´ì„œ ê¶ê¸ˆí•œ ê²ƒë“¤ì„ ë¬¼ì–´ë³´ì„¸ìš”."
        })

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = generate_response(prompt, clients)
                st.markdown(response)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()